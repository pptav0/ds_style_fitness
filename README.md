# ğŸš€ Candidate Fit Prediction with XGBoost & Bayesian Hyperparameter Tuning

## ğŸ” Why â€” Business Case
Organizations face a dual challenge when screening candidates:
- **Missed opportunities** when potential â€œFitâ€ candidates are incorrectly rejected (false negatives).
- **Wasted resources** when unsuitable â€œNot Fitâ€ candidates are misclassified as â€œFitâ€ (false positives).

Both outcomes reduce efficiency, increase costs, and create operational risks.
A reliable predictive model helps decision-makers **prioritize the right candidates**, **optimize resources**, and **reduce risk**, ultimately maximizing business value.

---

## ğŸ› ï¸ How â€” Approach
This project applies a combination of **advanced machine learning** and **Bayesian optimization**:

1. **Modeling**
   - Used **XGBoost**, a state-of-the-art gradient boosting classifier, well-suited for structured/tabular data.

2. **Hyperparameter Optimization**
   - Applied **Bayesian Probabilistic Hyperparameter Tuning with Optuna**.
   - Efficiently searches the parameter space, avoiding wasteful grid/random searches.

3. **Objective Function**
   - Custom-defined to balance **AUC, precision, recall, and F1**.
   - Ensures alignment with **business priorities** (capturing true â€œFitsâ€ while filtering out â€œNot Fitsâ€).

4. **Evaluation**
   - **Cross-validation** for robust estimates.
   - **ROC-AUC curves** to assess classification power.
   - **Precision-Recall curves** to evaluate trade-offs under class imbalance.

---

## ğŸ“Š What â€” Results
The tuned XGBoost model delivered **clear improvements** over the baseline:

- **AUC** and **Accuracy** increased â†’ confirming stronger generalization.
- **Class 1 (â€œFitâ€)**: Recall and F1 improved â†’ fewer missed opportunities.
- **Class 0 (â€œNot Fitâ€)**: Precision increased â†’ fewer wasted resources.
- **Macro F1** and **Weighted F1** gains â†’ balanced improvements across both classes.

**In summary**:
By combining **XGBoost** with **Bayesian hyperparameter tuning**, the project demonstrates how data science can deliver **direct business value**:
- Higher efficiency
- Lower risk
- Smarter, more confident decision-making

---

## ğŸ“‚ Repository Structure
style_fitness/
â”‚â”€â”€ data/ 										# Datasets (raw/processed)
â”‚
â”‚â”€â”€ style_fitness/
â”‚ â”œâ”€â”€ helpers/
â”‚ â”‚ â”œâ”€â”€ data_handler.py 			# Dataset handling and transformations
â”‚ â”‚ â”œâ”€â”€ data_preparation.py 	# Preprocessing pipeline
â”‚ â”‚ â”œâ”€â”€ plots.py 							# Custom visualization helpers (ROC, PR curves)
â”‚ â”‚ â””â”€â”€ init.py
â”‚ â”‚
â”‚ â”œâ”€â”€ notebooks/ 									# Jupyter notebooks for experimentation
â”‚ â”‚ â”œâ”€â”€ project_analysis.ipynb 		# Model analysis
â”‚ â”‚ â””â”€â”€ init.py
â”‚ â”‚
â”‚ â””â”€â”€ init.py
â”‚
â”‚â”€â”€ .gitignore
â”‚â”€â”€ poetry.lock 				# Dependency lockfile
â”‚â”€â”€ pyproject.toml 			# Project dependencies & configuration
â”‚â”€â”€ README.md 					# Project documentation


---

## ğŸ“ˆ Visuals
- **ROC Curves** â†’ better separation of Fit vs Not Fit
- **Precision-Recall Curves** â†’ higher precision across thresholds
- **Comparison Tables** â†’ percentage improvements vs. baseline

---

## âœ… Conclusion
This project shows how combining **machine learning (XGBoost)** with **Bayesian optimization (Optuna)** enables the development of **robust, business-aligned predictive models**.

The tuned model improves both efficiency and confidence:
- More true â€œFitâ€ candidates are captured.
- â€œNot Fitâ€ candidates are filtered with higher precision.
- Resources are directed to the right opportunities with **lower misclassification risk**.

---

## ğŸš€ Quick Start

```bash
# Clone the repository
git clone https://github.com/yourusername/style_fitness.git
cd style_fitness

# Install dependencies (using Poetry)
poetry install
